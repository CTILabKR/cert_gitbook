# 5.5. 웹 크롤링

웹 크롤러(web crawler)는 조직적, 자동화된 방법으로 인터넷에 있는 웹페이지를 방문해서 자료를 수집하는 일을 하는 프로그램을 말합니다. 웹 크롤러가 하는 작업을 '웹 크롤링'(web crawling) 혹은 '스파이더링'(spidering)이라 부릅니다. 검색 엔진과 같은 여러 사이트에서는 데이터의 최신 상태 유지를 위해 웹 크롤링합니다. 웹 크롤러는 대체로 방문한 사이트의 모든 페이지의 복사본을 생성하는 데 사용되며, 검색 엔진은 이렇게 생성된 페이지를 보다 빠른 검색을 위해 인덱싱합니다. 또한 크롤러는 링크 체크나 HTML 코드 검증과 같은 웹 사이트의 자동 유지 관리 작업을 위해 사용되기도 하며, 자동 이메일 수집과 같은 웹 페이지의 특정 형태의 정보를 수집하는 데도 사용됩니다.

웹 크롤러는 봇이나 소프트웨어 에이전트의 한 형태이다. 웹 크롤러는 대개 시드(seeds)라고 불리는 URL 리스트에서부터 시작하는데, 페이지의 모든 하이퍼링크를 인식하여 URL 리스트를 갱신합니다. 갱신된 URL 리스트는 재귀적으로 다시 방문합니다. 이때 한 페이지만 방문하는 것이 아니라 그 페이지에 링크되어 있는 또 다른 페이지를 차례대로 방문하고 이처럼 링크를 따라 웹을 돌아다니는 모습이 마치 거미와 비슷하다고 해서 스파이더라고 부르기도 합니다. 엄청난 분량의 웹문서를 사람이 일일 구별해서 모으는 일은 불가능에 가깝습니다. 때문에 웹 문서 검색에서는 사람이 일일이 하는 대신 이를 자동으로 수행해 줍니다. 웹 크롤러 크롤링의 다른 명칭은 \*\*'Web Scraping', ‘web harvesting’, ‘web data extraction’\*\*입니다.

머신러닝, 딥러닝에서 학습데이터를 구하기 위해 빅데이터, 데이터 분석에 대한 수요가 증가하고 이에 따라 자료를 얻는 원천으로 웹을 많이 이용합니다. 웹에서 데이터를 수집하고 분석을 위한 형태로 자료의 형태로 바꾸는 것이 바로 웹 크롤링입니다.

파이썬에서 웹크롤링을 하기 위해서는 BeautifulSoup, Selenium과 같은 도구를 사용해야 합니다. BeautifulSoup 은 HTML 및 XML 파일에서 원하는 데이터를 손쉽게 Parsing 할 수 있는 Python 라이브러리입니다. 웹문서의 구조를 찾아내는 파서를 이용해 찾고자 하는 데이터의 위치를 찾아 내어 값을 추출합니다.

예제는 [https://beomi.github.io/gb-crawling/](https://beomi.github.io/gb-crawling/) 싸이트를 참고 하였습니다.
